<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>PythonéŸ³é¢‘å¤„ç†ç¤ºä¾‹ - è®¡ç®—æœºéŸ³é¢‘å¤„ç†å­¦ä¹ æŒ‡å— </title>
    <link rel="stylesheet" href="css/style.css">
</head>
<body>
    <aside class="sidebar">
        <div class="sidebar-header">
            <h2>ğŸµ éŸ³é¢‘æ•™ç¨‹</h2>
            <p>è®¡ç®—æœºéŸ³é¢‘å¤„ç†å­¦ä¹ æŒ‡å—</p>
        </div>
        <ul class="nav-tree">
            <li><a href="index.html"><span class="nav-icon">ğŸ”Š</span>éŸ³é¢‘åŸºç¡€</a></li>
            <li><a href="algorithms.html"><span class="nav-icon">âš™ï¸</span>å¸¸ç”¨ç®—æ³•</a></li>
            <li><a href="ai-audio.html"><span class="nav-icon">ğŸ¤–</span>AI éŸ³é¢‘å¤„ç†</a></li>
            <li><a href="streaming-protocols.html"><span class="nav-icon">ğŸ“¡</span>åœ¨çº¿éŸ³é¢‘åè®®</a></li>
            <li><a href="audio-terms.html"><span class="nav-icon">ğŸ“–</span>éŸ³é¢‘æœ¯è¯­</a></li>
            <li><a href="neural-codec.html"><span class="nav-icon">ğŸ§ </span>ç¥ç»ç¼–è§£ç å™¨</a></li>
            <li><a href="python-examples.html" class="active"><span class="nav-icon">ğŸ</span>Python ç¤ºä¾‹</a></li>
            <li><a href="golang-examples.html"><span class="nav-icon">ğŸ’»</span>Golang ç¤ºä¾‹</a></li>
            <li><a href="ffmpeg-audio.html"><span class="nav-icon">ğŸ¬</span>FFmpeg å¤„ç†</a></li>
        </ul>
    </aside>

    <main class="main-content">
        <section class="section">
            <h2>ğŸ“¦ ç¯å¢ƒå‡†å¤‡ (ç‰ˆ)</h2>
            
            <div class="content-block">
                <h4>å®‰è£…æ‰€éœ€åº“</h4>
                <div class="code-block">
                    <pre><span class="comment"># ä¼ ç»ŸéŸ³é¢‘å¤„ç†</span>
pip install numpy scipy matplotlib librosa soundfile pyaudio

<span class="comment"># AI/æ·±åº¦å­¦ä¹ éŸ³é¢‘å¤„ç† (æ¨è)</span>
pip install torch torchaudio torchvision
pip install transformers datasets
pip install speechbrain
pip install audiolm-pytorch audiocraft
pip install onnxruntime onnx

<span class="comment"># å®æ—¶éŸ³é¢‘å¤„ç†</span>
pip install sounddevice noisereduce
</pre>
                </div>
            </div>

            <div class="info-box">
                <h4>å¹´æ¨èç¯å¢ƒ</h4>
                <ul>
                    <li><strong>PyTorch 2.5+</strong>ï¼šæ”¯æŒGPUåŠ é€Ÿå’Œé‡åŒ–æ¨ç†</li>
                    <li><strong>Transformers</strong>ï¼šHuggingFaceé¢„è®­ç»ƒæ¨¡å‹åº“</li>
                    <li><strong>SpeechBrain</strong>ï¼šç«¯åˆ°ç«¯è¯­éŸ³å¤„ç†</li>
                    <li><strong>ONNX Runtime</strong>ï¼šè·¨å¹³å°AIæ¨ç†</li>
                </ul>
            </div>
        </section>

        <section class="section">
            <h2>ğŸµ ç¤ºä¾‹1ï¼šAIè¯­éŸ³è¯†åˆ« (Whisper)</h2>
            
            <div class="example-section">
                <h3>ä½¿ç”¨OpenAI Whisperè¿›è¡Œè¯­éŸ³è¯†åˆ«</h3>
                
                <div class="language-tabs">
                    <button class="language-tab python active" onclick="showCode('python1')">Python</button>
                </div>
                
                <div id="python1" class="code-block">
                    <pre><span class="keyword">import</span> torch
<span class="keyword">import</span> torchaudio
<span class="keyword">from</span> transformers <span class="keyword">import</span> WhisperProcessor, WhisperForConditionalGeneration
<span class="keyword">import</span> numpy <span class="keyword">as</span> np

<span class="keyword">class</span> <span class="function">AIWhisperRecognizer</span>:
    <span class="string">"""
    ç‰ˆAIè¯­éŸ³è¯†åˆ«å™¨
    ä½¿ç”¨OpenAI Whisperæ¨¡å‹è¿›è¡Œé«˜ç²¾åº¦è¯­éŸ³è¯†åˆ«
    """</span>
    
    <span class="keyword">def</span> <span class="function">__init__</span>(self, model_size=<span class="string">"medium"</span>, device=<span class="string">"cuda"</span>):
        <span class="string">"""
        åˆå§‹åŒ–Whisperæ¨¡å‹
        
        å‚æ•°:
            model_size: æ¨¡å‹å¤§å° ("tiny", "base", "small", "medium", "large")
            device: è¿è¡Œè®¾å¤‡ ("cuda", "cpu", "mps")
        """</span>
        self.device = torch.device(device <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">"cpu"</span>)
        print(f<span class="string">"Loading Whisper {model_size} on {self.device}..."</span>)
        
        self.processor = WhisperProcessor.from_pretrained(
            <span class="string">f"openai/whisper-{model_size}"</span>
        )
        self.model = WhisperForConditionalGeneration.from_pretrained(
            <span class="string">f"openai/whisper-{model_size}"</span>
        )
        self.model.to(self.device)
        self.model.config.forced_decoder_ids = <span class="keyword">None</span>
        
    <span class="keyword">def</span> <span class="function">transcribe</span>(self, audio_path, language=<span class="string">"zh"</span>):
        <span class="string">"""
        è½¬å½•éŸ³é¢‘æ–‡ä»¶
        
        å‚æ•°:
            audio_path: éŸ³é¢‘æ–‡ä»¶è·¯å¾„
            language: è¯­è¨€ ("zh" for Chinese, "en" for English)
            
        è¿”å›:
            dict: è½¬å½•ç»“æœï¼ŒåŒ…å«æ–‡æœ¬å’Œæ—¶é—´æˆ³
        """</span>
        <span class="comment"># åŠ è½½éŸ³é¢‘</span>
        waveform, sample_rate = torchaudio.load(audio_path)
        
        <span class="comment"># é‡é‡‡æ ·åˆ°16kHz</span>
        <span class="keyword">if</span> sample_rate != <span class="number">16000</span>:
            resampler = torchaudio.transforms.Resample(sample_rate, <span class="number">16000</span>)
            waveform = resampler(waveform)
        
        <span class="comment"># é¢„å¤„ç†</span>
        input_features = self.processor(
            waveform.squeeze().numpy(),
            sampling_rate=<span class="number">16000</span>,
            return_tensors=<span class="string">"pt"</span>
        ).input_features
        
        <span class="comment"># æ¨ç†</span>
        self.model.eval()
        <span class="keyword">with</span> torch.no_grad():
            predicted_ids = self.model.generate(
                input_features.to(self.device),
                language=language
            )
        
        <span class="comment"># è§£ç </span>
        transcription = self.processor.batch_decode(
            predicted_ids, skip_special_tokens=<span class="keyword">True</span>
        )[<span class="number">0</span>]
        
        <span class="keyword">return</span> {
            <span class="string">"text"</span>: transcription,
            <span class="string">"language"</span>: language,
            <span class="string">"model"</span>: <span class="string">f"whisper-{self.model.config.model_name_or_path.split('-')[-1]}"</span>
        }

<span class="comment"># ä½¿ç”¨ç¤ºä¾‹</span>
<span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:
    recognizer = AIWhisperRecognizer(model_size=<span class="string">"medium"</span>, device=<span class="string">"cuda"</span>)
    
    <span class="comment"># è½¬å½•éŸ³é¢‘</span>
    result = recognizer.transcribe(<span class="string">"speech.wav"</span>, language=<span class="string">"zh"</span>)
    print(<span class="string">f"è½¬å½•ç»“æœ: {result['text']}"</span>)</pre>
                </div>
            </div>
        </section>

        <section class="section">
            <h2">ğŸµ ç¤ºä¾‹2ï¼šAIè¯­éŸ³å¢å¼º (RNNoise)</h2>
            
            <div class="example-section">
                <h3>ä½¿ç”¨æ·±åº¦å­¦ä¹ è¿›è¡Œå®æ—¶è¯­éŸ³é™å™ª</h3>
                
                <div class="language-tabs">
                    <button class="language-tab python active" onclick="showCode('python2')">Python</button>
                </div>
                
                <div id="python2" class="code-block">
                    <pre><span class="keyword">import</span> numpy <span class="keyword">as</span> np
<span class="keyword">import</span> torch
<span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn
<span class="keyword">import</span> torchaudio

<span class="keyword">class</span> <span class="function">Denoiser</span>(nn.Module):
    <span class="string">"""
    ç‰ˆè½»é‡çº§è¯­éŸ³é™å™ªæ¨¡å‹
    åŸºäºæ—¶åŸŸå·ç§¯ç½‘ç»œçš„å®æ—¶é™å™ª
    """</span>
    
    <span class="keyword">def</span> <span class="function">__init__</span>(self):
        <span class="keyword">super</span>(Denoiser, self).__init__()
        
        <span class="comment"># ç¼–ç å™¨</span>
        self.encoder = nn.Sequential(
            nn.Conv1d(<span class="number">1</span>, <span class="number">32</span>, <span class="number">8</span>, stride=<span class="number">4</span>, padding=<span class="number">2</span>),
            nn.ReLU(),
            nn.Conv1d(<span class="number">32</span>, <span class="number">64</span>, <span class="number">8</span>, stride=<span class="number">4</span>, padding=<span class="number">2</span>),
            nn.ReLU(),
            nn.Conv1d(<span class="number">64</span>, <span class="number">128</span>, <span class="number">8</span>, stride=<span class="number">4</span>, padding=<span class="number">2</span>),
            nn.ReLU(),
            nn.Conv1d(<span class="number">128</span>, <span class="number">256</span>, <span class="number">8</span>, stride=<span class="number">4</span>, padding=<span class="number">2</span>),
        )
        
        <span class="comment"># è§£ç å™¨</span>
        self.decoder = nn.Sequential(
            nn.ConvTranspose1d(<span class="number">256</span>, <span class="number">128</span>, <span class="number">8</span>, stride=<span class="number">4</span>, padding=<span class="number">2</span>),
            nn.ReLU(),
            nn.ConvTranspose1d(<span class="number">128</span>, <span class="number">64</span>, <span class="number">8</span>, stride=<span class="number">4</span>, padding=<span class="number">2</span>),
            nn.ReLU(),
            nn.ConvTranspose1d(<span class="number">64</span>, <span class="number">32</span>, <span class="number">8</span>, stride=<span class="number">4</span>, padding=<span class="number">2</span>),
            nn.ReLU(),
            nn.ConvTranspose1d(<span class="number">32</span>, <span class="number">1</span>, <span class="number">8</span>, stride=<span class="number">4</span>, padding=<span class="number">2</span>),
            nn.Tanh()
        )
        
    <span class="keyword">def</span> <span class="function">forward</span>(self, x):
        <span class="string">"""
        å‰å‘ä¼ æ’­
        
        å‚æ•°:
            x: è¾“å…¥éŸ³é¢‘ tensor [batch, 1, samples]
            
        è¿”å›:
            å¢å¼ºåçš„éŸ³é¢‘
        """</span>
        encoded = self.encoder(x)
        decoded = self.decoder(encoded)
        <span class="keyword">return</span> decoded

<span class="keyword">class</span> <span class="function">RealTimeNoiseSuppressor</span>:
    <span class="string">"""
    å®æ—¶è¯­éŸ³é™å™ªå¤„ç†å™¨
    å¹´ä¼˜åŒ–ç‰ˆï¼Œæ”¯æŒæµå¼å¤„ç†
    """</span>
    
    <span class="keyword">def</span> <span class="function">__init__</span>(self, model_path=<span class="keyword">None</span>, sample_rate=<span class="number">16000</span>):
        <span class="string">"""
        åˆå§‹åŒ–é™å™ªå™¨
        
        å‚æ•°:
            model_path: é¢„è®­ç»ƒæ¨¡å‹è·¯å¾„
            sample_rate: é‡‡æ ·ç‡
        """</span>
        self.sample_rate = sample_rate
        self.model = Denoiser()
        
        <span class="keyword">if</span> model_path:
            self.model.load_state_dict(torch.load(model_path))
        
        self.model.eval()
        self.device = torch.device(<span class="string">"cuda"</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">"cpu"</span>)
        self.model.to(self.device)
        
        <span class="comment"># ç¼“å­˜åŒº</span>
        self.buffer = torch.zeros(<span class="number">1</span>, <span class="number">1</span>, <span class="number">16000</span> * <span class="number">2</span>)  <span class="comment"># 2ç§’ç¼“å­˜</span>
        
    <span class="keyword">def</span> <span class="function">process_chunk</span>(self, audio_chunk):
        <span class="string">"""
        å¤„ç†éŸ³é¢‘å—ï¼ˆå®æ—¶æ¨¡å¼ï¼‰
        
        å‚æ•°:
            audio_chunk: numpyæ•°ç»„æˆ–tensor
            
        è¿”å›:
            å¢å¼ºåçš„éŸ³é¢‘
        """</span>
        <span class="keyword">if</span> isinstance(audio_chunk, np.ndarray):
            audio_chunk = torch.from_numpy(audio_chunk).float().unsqueeze(<span class="number">0</span>).unsqueeze(<span class="number">0</span>)
        
        <span class="keyword">with</span> torch.no_grad():
            enhanced = self.model(audio_chunk.to(self.device))
        
        <span class="keyword">return</span> enhanced.cpu().squeeze().numpy()

<span class="comment"># ä½¿ç”¨ç¤ºä¾‹</span>
<span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:
    <span class="comment"># åˆ›å»ºé™å™ªå™¨</span>
    suppressor = RealTimeNoiseSuppressor()
    
    <span class="comment"># å¤„ç†éŸ³é¢‘æ–‡ä»¶</span>
    waveform, sr = torchaudio.load(<span class="string">"noisy_speech.wav"</span>)
    enhanced = suppressor.process_chunk(waveform)
    
    <span class="comment"># ä¿å­˜ç»“æœ</span>
    torchaudio.save(<span class="string">"enhanced_speech.wav"</span>, 
                   torch.from_numpy(enhanced).unsqueeze(<span class="number">0</span>), sr)</pre>
                </div>
            </div>
        </section>

        <section class="section">
            <h2">ğŸµ ç¤ºä¾‹3ï¼šAIéŸ³ä¹ç”Ÿæˆ (MusicGen)</h2>
            
            <div class="example-section">
                <h3>ä½¿ç”¨Meta MusicGenç”ŸæˆéŸ³ä¹</h3>
                
                <div class="language-tabs">
                    <button class="language-tab python active" onclick="showCode('python3')">Python</button>
                </div>
                
                <div id="python3" class="code-block">
                    <pre><span class="keyword">import</span> torch
<span class="keyword">import</span> torchaudio
<span class="keyword">from</span> audiocraft.models <span class="keyword">import</span> MusicGen
<span class="keyword">from</span> audiocraft.utils <span class="keyword">import</span> play_audio

<span class="keyword">class</span> <span class="function">AIMusicGenerator</span>:
    <span class="string">"""
    ç‰ˆAIéŸ³ä¹ç”Ÿæˆå™¨
    ä½¿ç”¨Meta MusicGenæ ¹æ®æ–‡æœ¬æè¿°ç”ŸæˆéŸ³ä¹
    """</span>
    
    <span class="keyword">def</span> <span class="function">__init__</span>(self, model_size=<span class="string">"medium"</span>):
        <span class="string">"""
        åˆå§‹åŒ–éŸ³ä¹ç”Ÿæˆæ¨¡å‹
        
        å‚æ•°:
            model_size: æ¨¡å‹å¤§å° ("small", "medium", "large", "melody")
        """</span>
        print(f<span class="string">"Loading MusicGen {model_size}..."</span>)
        self.model = MusicGen.get_pretrained(model_size)
        self.device = <span class="string">"cuda"</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">"cpu"</span>
        self.model.to(self.device)
        
    <span class="keyword">def</span> <span class="function">generate</span>(self, description, duration=<span class="number">10</span>, tempo=<span class="keyword">None</span>):
        <span class="string">"""
        æ ¹æ®æ–‡æœ¬æè¿°ç”ŸæˆéŸ³ä¹
        
        å‚æ•°:
            description: éŸ³ä¹æè¿° (è‹±æ–‡)
                ç¤ºä¾‹: "a relaxing piano melody with soft strings"
                ç¤ºä¾‹: "electronic dance music with strong beat"
            duration: ç”Ÿæˆæ—¶é•¿ï¼ˆç§’ï¼‰
            tempo: BPM (å¯é€‰)
            
        è¿”å›:
            ç”Ÿæˆçš„éŸ³é¢‘tensor
        """</span>
        self.model.set_generation_params(
            duration=duration,
            tempo=tempo
        )
        
        <span class="keyword">with</span> torch.no_grad():
            audio = self.model.generate([
                description
            ])
        
        <span class="keyword">return</span> audio.cpu()
    
    <span class="keyword">def</span> <span class="function">generate_melody_control</span>(self, melody_audio_path, duration=<span class="number">10</span>):
        <span class="string">"""
        åŸºäºå‚è€ƒæ—‹å¾‹ç”ŸæˆéŸ³ä¹
        
        å‚æ•°:
            melody_audio_path: å‚è€ƒæ—‹å¾‹éŸ³é¢‘è·¯å¾„
            duration: ç”Ÿæˆæ—¶é•¿
            
        è¿”å›:
            ç”Ÿæˆçš„éŸ³é¢‘tensor
        """</span>
        <span class="comment"># åŠ è½½å‚è€ƒæ—‹å¾‹</span>
        melody, sr = torchaudio.load(melody_audio_path)
        
        self.model.set_generation_params(duration=duration)
        
        <span class="keyword">with</span> torch.no_grad():
            audio = self.model.generate_with_chroma(
                [melody],
                description=<span class="string">"continue the melody style"</span>
            )
        
        <span class="keyword">return</span> audio.cpu()
    
    <span class="keyword">def</span> <span class="function">save_audio</span>(self, audio_tensor, output_path, sample_rate=<span class="number">32000</span>):
        <span class="string">"""
        ä¿å­˜ç”Ÿæˆçš„éŸ³é¢‘
        """</span>
        torchaudio.save(output_path, audio_tensor.squeeze(), sample_rate)

<span class="comment"># ä½¿ç”¨ç¤ºä¾‹</span>
<span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:
    <span class="comment"># åˆå§‹åŒ–ç”Ÿæˆå™¨</span>
    generator = AIMusicGenerator(model_size=<span class="string">"medium"</span>)
    
    <span class="comment"># ç”ŸæˆéŸ³ä¹</span>
    prompt = <span class="string">"a calm ambient electronic music with soft pads and gentle rhythm"</span>
    audio = generator.generate(prompt, duration=<span class="number">15</span>, tempo=<span class="number">80</span>)
    
    <span class="comment"># ä¿å­˜</span>
    generator.save_audio(audio, <span class="string">"generated_music.wav"</span>)
    
    <span class="comment"># æ’­æ”¾</span>
    play_audio(audio.squeeze())</pre>
                </div>
            </div>
        </section>

        <section class="section">
            <h2">ğŸµ ç¤ºä¾‹4ï¼šAIéŸ³é¢‘ç‰¹å¾æå– (Wav2Vec 2.0)</h2>
            
            <div class="example-section">
                <h3>ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹æå–é«˜çº§éŸ³é¢‘è¡¨ç¤º</h3>
                
                <div class="language-tabs">
                    <button class="language-tab python active" onclick="showCode('python4')">Python</button>
                </div>
                
                <div id="python4" class="code-block">
                    <pre><span class="keyword">import</span> torch
<span class="keyword">import</span> torchaudio
<span class="keyword">import</span> numpy <span class="keyword">as</span> np
<span class="keyword">from</span> transformers <span class="keyword">import</span> Wav2Vec2Processor, Wav2Vec2Model
<span class="keyword">from</span> sklearn.decomposition <span class="keyword">import</span> PCA

<span class="keyword">class</span> <span class="function">AudioFeatureExtractor</span>:
    <span class="string">"""
    ç‰ˆé«˜çº§éŸ³é¢‘ç‰¹å¾æå–å™¨
    ä½¿ç”¨Wav2Vec 2.0é¢„è®­ç»ƒæ¨¡å‹æå–éŸ³é¢‘åµŒå…¥
    """</span>
    
    <span class="keyword">def</span> <span class="function">__init__</span>(self, model_name=<span class="string">"facebook/wav2vec2-base-960h"</span>):
        <span class="string">"""
        åˆå§‹åŒ–ç‰¹å¾æå–å™¨
        
        å‚æ•°:
            model_name: HuggingFaceæ¨¡å‹åç§°
        """</span>
        self.processor = Wav2Vec2Processor.from_pretrained(model_name)
        self.model = Wav2Vec2Model.from_pretrained(model_name)
        self.device = <span class="string">"cuda"</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">"cpu"</span>
        self.model.to(self.device)
        self.model.eval()
        
    <span class="keyword">def</span> <span class="function">extract_features</span>(self, audio_path):
        <span class="string">"""
        æå–éŸ³é¢‘ç‰¹å¾åµŒå…¥
        
        å‚æ•°:
            audio_path: éŸ³é¢‘æ–‡ä»¶è·¯å¾„
            
        è¿”å›:
            audio_embeddings: å¸§çº§åˆ«åµŒå…¥
            sentence_embedding: å¥å­çº§åˆ«åµŒå…¥
        """</span>
        <span class="comment"># åŠ è½½éŸ³é¢‘</span>
        waveform, sample_rate = torchaudio.load(audio_path)
        
        <span class="comment"># é‡é‡‡æ ·åˆ°16kHz</span>
        <span class="keyword">if</span> sample_rate != <span class="number">16000</span>:
            resampler = torchaudio.transforms.Resample(sample_rate, <span class="number">16000</span>)
            waveform = resampler(waveform)
        
        <span class="comment"># é¢„å¤„ç†</span>
        input_values = self.processor(
            waveform.squeeze().numpy(),
            sampling_rate=<span class="number">16000</span>,
            return_tensors=<span class="string">"pt"</span>
        ).input_values
        
        <span class="comment">"># æ¨ç†</span>
        <span class="keyword">with</span> torch.no_grad():
            output = self.model(input_values.to(self.device))
            
        <span class="comment"># æå–åµŒå…¥</span>
        audio_embeddings = output.last_hidden_state.cpu()
        
        <span class="comment"># å…¨å±€å¹³å‡æ± åŒ–å¾—åˆ°å¥å­åµŒå…¥</span>
        sentence_embedding = torch.mean(audio_embeddings, dim=<span class="number">1</span>)
        
        <span class="keyword">return</span> {
            <span class="string">"frame_embeddings"</span>: audio_embeddings,
            <span class="string">"sentence_embedding"</span>: sentence_embedding,
            <span class="string">"sample_rate"</span>: sample_rate
        }
    
    <span class="keyword">def</span> <span class="function">similarity_search</span>(self, query_path, reference_paths):
        <span class="string">"""
        éŸ³é¢‘ç›¸ä¼¼åº¦æœç´¢
        
        å‚æ•°:
            query_path: æŸ¥è¯¢éŸ³é¢‘è·¯å¾„
            reference_paths: å‚è€ƒéŸ³é¢‘è·¯å¾„åˆ—è¡¨
            
        è¿”å›:
            ç›¸ä¼¼åº¦æ’åºåˆ—è¡¨
        """</span>
        <span class="comment"># æå–æŸ¥è¯¢éŸ³é¢‘åµŒå…¥</span>
        query_features = self.extract_features(query_path)
        query_embedding = query_features[<span class="string">"sentence_embedding"</span>]
        
        similarities = []
        <span class="keyword">for</span> ref_path <span class="keyword">in</span> reference_paths:
            ref_features = self.extract_features(ref_path)
            ref_embedding = ref_features[<span class="string">"sentence_embedding"</span>]
            
            <span class="comment"># ä½™å¼¦ç›¸ä¼¼åº¦</span>
            cos_sim = torch.nn.functional.cosine_similarity(
                query_embedding, ref_embedding
            ).item()
            
            similarities.append((ref_path, cos_sim))
        
        <span class="comment"># æ’åº</span>
        similarities.sort(key=<span class="keyword">lambda</span> x: x[<span class="number">1</span>], reverse=<span class="keyword">True</span>)
        
        <span class="keyword">return</span> similarities

<span class="comment"># ä½¿ç”¨ç¤ºä¾‹</span>
<span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:
    <span class="comment"># åˆå§‹åŒ–ç‰¹å¾æå–å™¨</span>
    extractor = AudioFeatureExtractor(
        model_name=<span class="string">"facebook/wav2vec2-base-960h"</span>
    )
    
    <span class="comment"># æå–ç‰¹å¾</span>
    features = extractor.extract_features(<span class="string">"speech.wav"</span>)
    print(f<span class="string">"Frame embeddings shape: {features['frame_embeddings'].shape}"</span>)
    print(f<span class="string">"Sentence embedding shape: {features['sentence_embedding'].shape}"</span>)</pre>
                </div>
            </div>
        </section>

        <section class="section">
            <h2">ğŸµ ç¤ºä¾‹5ï¼šAIéŸ³é¢‘åˆ†ç¦» (Demucs)</h2>
            
            <div class="example-section">
                <h3>ä½¿ç”¨Demucsè¿›è¡ŒéŸ³ä¹æºåˆ†ç¦»</h3>
                
                <div class="language-tabs">
                    <button class="language-tab python active" onclick="showCode('python5')">Python</button>
                </div>
                
                <div id="python5" class="code-block">
                    <pre><span class="keyword">import</span> torch
<span class="keyword">import</span> torchaudio
<span class="keyword">from</span> demucs <span class="keyword">import</span> pretrained
<span class="keyword">from</span> demucs.audio <span class="keyword">import</span> AudioFile

<span class="keyword">class</span> <span class="function">MusicSourceSeparator</span>:
    <span class="string">"""
    ç‰ˆAIéŸ³ä¹æºåˆ†ç¦»å™¨
    ä½¿ç”¨Demucsæ¨¡å‹å°†éŸ³ä¹åˆ†ç¦»ä¸ºäººå£°ã€ä¼´å¥ã€é¼“ã€è´æ–¯ç­‰
    """</span>
    
    <span class="keyword">def</span> <span class="function">__init__</span>(self, model=<span class="string">"demucs"</span>, device=<span class="string">"cuda"</span>):
        <span class="string">"""
        åˆå§‹åŒ–åˆ†ç¦»å™¨
        
        å‚æ•°:
            model: æ¨¡å‹é€‰æ‹©
                - "demucs": æ ‡å‡†ç‰ˆ
                - "demucs-mmi": æ›´ç²¾ç¡®ä½†æ›´æ…¢
                - "htdemucs": æœ€é«˜è´¨é‡
            device: è¿è¡Œè®¾å¤‡
        """</span>
        self.device = torch.device(device <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">"cpu"</span>)
        
        print(f<span class="string">"Loading {model} on {self.device}..."</span>)
        self.model = pretrained.get_model(model)
        self.model.to(self.device)
        
    <span class="keyword">def</span> <span class="function">separate</span>(self, audio_path, output_dir=<span class="string">"./"</span>):
        <span class="string">"""
        åˆ†ç¦»éŸ³ä¹æº
        
        å‚æ•°:
            audio_path: è¾“å…¥éŸ³ä¹æ–‡ä»¶è·¯å¾„
            output_dir: è¾“å‡ºç›®å½•
            
        è¿”å›:
            dict: åˆ†ç¦»åçš„å„éŸ³è½¨
        """</span>
        <span class="comment"># åŠ è½½éŸ³é¢‘</span>
        audio = AudioFile(audio_path).read(streams=<span class="number">0</span>)
        
        <span class="comment"># åˆ†ç¦»</span>
        <span class="keyword">with</span> torch.no_grad():
            estimates = self.model(audio.unsqueeze(<span class="number">0</span>).to(self.device))
        
        <span class="comment"># è·å–éŸ³è½¨åç§°</span>
        sources = self.model.sources
        
        <span class="keyword">return</span> {
            source: estimates[<span class="number">0</span>, i].cpu()
            <span class="keyword">for</span> i, source <span class="keyword">in</span> enumerate(sources)
        }
    
    <span class="keyword">def</span> <span class="function">save_stems</span>(self, stems, output_dir=<span class="string">"./"</span>, sample_rate=<span class="number">44100</span>):
        <span class="string">"""
        ä¿å­˜åˆ†ç¦»åçš„éŸ³è½¨
        """</span>
        <span class="keyword">for</span> name, audio <span class="keyword">in</span> stems.items():
            output_path = <span class="string">f"{output_dir}/{name}.wav"</span>
            torchaudio.save(output_path, audio, sample_rate)
            print(f<span class="string">"Saved: {output_path}"</span>)

<span class="comment"># ä½¿ç”¨ç¤ºä¾‹</span>
<span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:
    <span class="comment"># åˆå§‹åŒ–åˆ†ç¦»å™¨</span>
    separator = MusicSourceSeparator(model=<span class="string">"htdemucs"</span>, device=<span class="string">"cuda"</span>)
    
    <span class="comment"># åˆ†ç¦»éŸ³ä¹</span>
    stems = separator.separate(<span class="string">"song.wav"</span>)
    
    <span class="comment"># ä¿å­˜å„éŸ³è½¨</span>
    separator.save_stems(stems, output_dir=<span class="string">"separated"</span>)
    
    <span class="comment"># ä½¿ç”¨åˆ†ç¦»åçš„éŸ³è½¨</span>
    vocals = stems[<span class="string">"vocals"</span>]
    drums = stems[<span class="string">"drums"</span>]
    bass = stems[<span class="string">"bass"</span>]
    other = stems[<span class="string">"other"</span>]</pre>
                </div>
            </div>
        </section>

        <section class="section">
            <h2>ğŸµ ç¤ºä¾‹6ï¼šONNXæ¨¡å‹éƒ¨ç½²</h2>
            
            <div class="example-section">
                <h3>è·¨å¹³å°AIéŸ³é¢‘æ¨¡å‹éƒ¨ç½²</h3>
                
                <div class="language-tabs">
                    <button class="language-tab python active" onclick="showCode('python6')">Python</button>
                </div>
                
                <div id="python6" class="code-block">
                    <pre><span class="keyword">import</span> torch
<span class="keyword">import</span> onnxruntime <span class="keyword">as</span> ort
<span class="keyword">import</span> numpy <span class="keyword">as</span> np
<span class="keyword">import</span> torchaudio

<span class="keyword">class</span> <span class="function">ONNXAudioProcessor</span>:
    <span class="string">"""
    ç‰ˆONNXéŸ³é¢‘å¤„ç†å¼•æ“
    æ”¯æŒè·¨å¹³å°éƒ¨ç½²å’Œç¡¬ä»¶åŠ é€Ÿæ¨ç†
    """</span>
    
    <span class="keyword">def</span> <span class="function">__init__</span>(self, model_path, providers=[<span class="string">"CUDAExecutionProvider"</span>, 
                                              <span class="string">"CPUExecutionProvider"</span>]):
        <span class="string">"""
        åˆå§‹åŒ–ONNXæ¨ç†å¼•æ“
        
        å‚æ•°:
            model_path: ONNXæ¨¡å‹æ–‡ä»¶è·¯å¾„
            providers: æ‰§è¡Œæä¾›ç¨‹åº (CUDA, TensorRT, CoreML, NPUç­‰)
        """</span>
        <span class="comment"># æ£€æŸ¥å¯ç”¨çš„providers</span>
        available = ort.get_available_providers()
        providers = [p <span class="keyword">for</span> p <span class="keyword">in</span> providers <span class="keyword">if</span> p <span class="keyword">in</span> available]
        
        print(f<span class="string">"Using providers: {providers}"</span>)
        self.session = ort.InferenceSession(model_path, providers=providers)
        
        <span class="comment">"># è·å–æ¨¡å‹ä¿¡æ¯</span>
        self.input_name = self.session.get_inputs()[<span class="number">0</span>].name
        self.output_names = [o.name <span class="keyword">for</span> o <span class="keyword">in</span> self.session.get_outputs()]
        
    <span class="keyword">def</span> <span class="function">infer</span>(self, input_data):
        <span class="string">"""
        æ‰§è¡Œæ¨ç†
        
        å‚æ•°:
            input_data: è¾“å…¥éŸ³é¢‘tensoræˆ–numpyæ•°ç»„
            
        è¿”å›:
            æ¨¡å‹è¾“å‡º
        """</span>
        <span class="keyword">if</span> isinstance(input_data, torch.Tensor):
            input_data = input_data.numpy()
        
        <span class="comment"># æ‰§è¡Œæ¨ç†</span>
        outputs = self.session.run(self.output_names, {self.input_name: input_data})
        
        <span class="keyword">return</span> outputs

<span class="keyword">class</span> <span class="function">ModelConverter</span>:
    <span class="string">"""
    PyTorchæ¨¡å‹è½¬ONNX
    """</span>
    
    <span class="keyword">def</span> <span class="function">__init__</span>(self, torch_model):
        self.model = torch_model
        
    <span class="keyword">def</span> <span class="function">export</span>(self, dummy_input, output_path, input_names=[<span class="string">"audio"</span>],
                   output_names=[<span class="string">"output"</span>], dynamic_axes=None):
        <span class="string">"""
        å¯¼å‡ºONNXæ¨¡å‹
        
        å‚æ•°:
            dummy_input: ç¤ºä¾‹è¾“å…¥
            output_path: è¾“å‡ºè·¯å¾„
            input_names: è¾“å…¥åç§°
            output_names: è¾“å‡ºåç§°
            dynamic_axes: åŠ¨æ€è½´ï¼ˆæ”¯æŒå˜é•¿è¾“å…¥ï¼‰
        """</span>
        torch.onnx.export(
            self.model,
            dummy_input,
            output_path,
            input_names=input_names,
            output_names=output_names,
            dynamic_axes=dynamic_axes,
            opset_version=<span class="number">17</span>  <span class="comment"># ONNX Opset 17 ()
        )
        print(f<span class="string">"Exported to: {output_path}"</span>)

<span class="comment"># ä½¿ç”¨ç¤ºä¾‹</span>
<span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:
    <span class="comment"># è½¬æ¢PyTorchæ¨¡å‹ä¸ºONNX</span>
    class SimpleAudioModel(torch.nn.Module):
        <span class="keyword">def</span> forward(self, x):
            <span class="keyword">return</span> x * <span class="number">0.5</span>  <span class="comment"># ç®€å•ç¤ºä¾‹
    
    model = SimpleAudioModel()
    converter = ModelConverter(model)
    
    dummy_input = torch.randn(<span class="number">1</span>, <span class="number">16000</span>)  <span class="comment"># 1ç§’éŸ³é¢‘
    converter.export(
        dummy_input,
        "audio_model.onnx",
        input_names=[<span class="string">"waveform"</span>],
        output_names=[<span class="string">"output"</span>],
        dynamic_axes={<span class="string">"waveform"</span>: {<span class="number">0</span>: <span class="string">"batch"</span>, <span class="number">1</span>: <span class="string">"samples"</span>}}
    )
    
    <span class="comment"># ä½¿ç”¨ONNXæ¨¡å‹æ¨ç†</span>
    engine = ONNXAudioProcessor(<span class="string">"audio_model.onnx"</span>)
    test_audio = np.random.randn(<span class="number">16000</span>).astype(np.float32)
    output = engine.infer(test_audio)</pre>
                </div>
            </div>
        </section>

    <a href="#" class="back-to-top">â†‘ è¿”å›é¡¶éƒ¨</a>
    </main>

</body>
</html>
